{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58219d",
   "metadata": {},
   "source": [
    "## Part 1: Off-Policy Evaluation: Warm-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f044a84",
   "metadata": {},
   "source": [
    "The following code generates logged data from a \"tabular\" policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd2d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_data_tabular(n, num_contexts, num_actions, seed=1):\n",
    "    '''\n",
    "    tabular: Uniform distribution on contexts. Distribution on actions chosen randomly from the simplex on actions.\n",
    "    The random seed is set to 10 to ensure the parameters are similar each round. It is then set to seed to ensure randomization\n",
    "    before rounds\n",
    "    \n",
    "    Returns\n",
    "    - df: dataframe with n rows and columns: context, action, propensity, reward  \n",
    "    - r: num_contexts x num_actions: reward matrix\n",
    "    - pi0: num_contexts x num_actions: matrix where pi0[i,j] = pi0(a_j|x_i)\n",
    "    '''\n",
    "    np.random.seed(10)\n",
    "    r = np.random.rand(num_contexts, num_actions)\n",
    "    pi0 = np.random.rand(num_contexts, num_actions)\n",
    "    pi0 = pi0/pi0.sum(axis=1)[:,np.newaxis]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    contexts = list(range(num_contexts))\n",
    "    actions = list(range(num_actions))\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        ct = contexts[np.random.choice(contexts)]\n",
    "        at = actions[np.random.choice(actions, p=pi0[ct])]\n",
    "        pt = pi0[ct,at]\n",
    "        rt = r[ct,at] + .1*np.random.randn()\n",
    "        data.append((ct,at,pt,rt, ))\n",
    "    df = pd.DataFrame(data, columns=['context', 'action', 'propensity', 'reward'])\n",
    "    df = df.astype(dtype={\"context\":\"int64\", \"action\":\"int64\"})\n",
    "    return df, r, pi0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64a880",
   "metadata": {},
   "source": [
    "I've also provided the following ``UniformPolicy`` class which samples each action with the same uniform probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90fed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformPolicy():\n",
    "    def __init__(self, num_contexts, num_actions):\n",
    "        self.num_contexts = num_contexts\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def pi_x(self, ctx):\n",
    "        '''\n",
    "        Return pi(-|x) which is a vector in R^|A|\n",
    "        '''\n",
    "        return np.ones(self.num_actions)/self.num_actions\n",
    "    \n",
    "    def pi_a_x(self, ctx, a):\n",
    "        '''\n",
    "        Return pi(a|x). \n",
    "        In the case of a deterministic policy should be a binary variable.\n",
    "        '''\n",
    "        return 1/self.num_actions\n",
    "    \n",
    "    def V(self, r):\n",
    "        '''\n",
    "        Return the true value of the task given the true rewards and \n",
    "        Assume context distribution is uniform.\n",
    "        '''\n",
    "        v = 0\n",
    "        for ctx in range(self.num_contexts):\n",
    "            for action in range(self.num_actions):\n",
    "                v += 1/self.num_contexts*self.pi_a_x(ctx, action)*r[ctx, action]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8812e",
   "metadata": {},
   "source": [
    "The following example shows an example of pulling a dataset and computing $V(\\pi_{unif})$ on the true reward function returned from the logging method above. Please run this code and understand the previous code fully before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeee9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d,r,pi0 = logging_data_tabular(10, 50, 2)\n",
    "uniform_policy = UniformPolicy(50,2)\n",
    "v_star = uniform_policy.V(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668e396",
   "metadata": {},
   "source": [
    "Please implement the IPS, direct method, and doubly robust estimators. You may assume that the policy object has the same methods as a ``UniformPolicy`` class above. I've given you the ``_rhat`` method which computes an empirical reward matrix given the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e957e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ips(policy, data, clip=0):\n",
    "    ## TODO implement IPS\n",
    "    return vhat_ips\n",
    "\n",
    "def _rhat(data, num_contexts, num_actions):\n",
    "    rhat = np.zeros((num_contexts, num_actions))\n",
    "    for ctx in range(num_contexts):\n",
    "        for action in range(num_actions):\n",
    "            dd = data.query(f'context=={ctx} and action=={action}')\n",
    "            rhat[ctx,action] = dd['reward'].sum()/max(len(dd), 1)\n",
    "    return rhat\n",
    "    \n",
    "def dm(policy, data):\n",
    "    ## TODO implement the direct method\n",
    "    ## You may want to use the _rhat method above\n",
    "    return vhat_dm\n",
    "\n",
    "def dr(policy, data, clip=0):\n",
    "   ## TODO implement the doubly robust estimator\n",
    "    return vhat_dr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d3ae9",
   "metadata": {},
   "source": [
    "Now run the simulation described in problem 2 of part 1 of the homework. I've tried to give you some boilerplate code. Feel free to modify anything below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068457b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 20\n",
    "ns = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "runs_ips = np.zeros((len(ns), reps))\n",
    "runs_dm = np.zeros((len(ns), reps))\n",
    "runs_dr = np.zeros((len(ns), reps))\n",
    "\n",
    "\n",
    "d,r,pi0 = logging_data_tabular(10, 50, 2)\n",
    "uniform_policy = UniformPolicy(50,2)\n",
    "v_star = uniform_policy.V(r)\n",
    "\n",
    "for j, n in enumerate(ns):    \n",
    "    for i in range(reps):\n",
    "        '''\n",
    "        TODO:\n",
    "        a) Draw a dataset of size n from logging_data_tabular\n",
    "        b) Compute vhat(pi_unif) using IPS, DM, DR estimators\n",
    "        c) Compute the mean squared error for each estimator relative to v_star\n",
    "        d) Store the resulting answer in runs_ips[j,i], runs_dm[j,i], runs_dr[j,i] respectively.\n",
    "        ''' \n",
    "        \n",
    "\n",
    "#plt.axhline(v_star, c='blue')\n",
    "plt.plot(ns, runs_ips.mean(axis=1), label='V_IPS', c='blue')\n",
    "plt.errorbar(ns, runs_ips.mean(axis=1), yerr=runs_ips.std(axis=1), c='blue')\n",
    "plt.plot(ns, runs_dm.mean(axis=1), label='V_dm',c='green')\n",
    "plt.errorbar(ns, runs_dm.mean(axis=1), yerr=runs_dm.std(axis=1),c='green')\n",
    "plt.plot(ns, runs_dr.mean(axis=1), label='V_dr',c='orange')\n",
    "plt.errorbar(ns, runs_dr.mean(axis=1), yerr=runs_dr.std(axis=1),c='orange')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110debd",
   "metadata": {},
   "source": [
    "## Part 2: Off-Policy Evaluation: Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511df4ed",
   "metadata": {},
   "source": [
    "The following code generates logged data from a \"softmax\" policy that is designed to be close to the true optimal deterministic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80202f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_data_logistic(n, num_contexts, num_actions, dim=2, seed=1, eta=.1):\n",
    "    '''\n",
    "    logistic: Please see homework for description.\n",
    "    '''\n",
    "    np.random.seed(10)\n",
    "    r = np.zeros((num_contexts, num_actions))\n",
    "    contexts = list(range(num_contexts))\n",
    "    actions = list(range(num_actions))\n",
    "    contexts_X = np.random.randn(num_contexts, dim)\n",
    "    contexts_A = np.random.randn(num_contexts, dim)\n",
    "    theta = np.random.rand(dim,dim)\n",
    "    for i in range(num_contexts):\n",
    "        for j in range(num_actions):\n",
    "            r[i,j] = max(contexts_X[i].T@theta@contexts_A[j],0)\n",
    "    pi0 = np.exp(eta*r+ .05*np.random.randn())\n",
    "    pi0 = pi0/pi0.sum(axis=1)[:,np.newaxis] \n",
    "    # policy - sample theta and look at probability given inner products of xa^top with theta \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        ct = contexts[np.random.choice(contexts)]\n",
    "        at = actions[np.random.choice(actions, p=pi0[ct])]\n",
    "        pt = pi0[ct,at]\n",
    "        rt = max(r[ct,at] + .1*np.random.randn(), 0)\n",
    "        data.append((ct,at,pt,rt))\n",
    "    data.append((ct,at,pt,rt))\n",
    "    df = pd.DataFrame(data, columns=['context', 'action', 'propensity', 'reward'])\n",
    "    df = df.astype(dtype={\"context\":\"int64\", \"action\":\"int64\"})\n",
    "    return df, r, pi0, contexts_X, contexts_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc1c84",
   "metadata": {},
   "source": [
    "I've also provided the ``LogisticDeterministicPolicy`` class which provides a wrapper class to convert a multi-class classifier into a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35c34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticDeterministicPolicy():\n",
    "    def __init__(self, num_contexts, num_actions, model, contexts_X, contexts_A):\n",
    "        self.num_contexts = num_contexts\n",
    "        self.num_actions = num_actions\n",
    "        self.model = model\n",
    "    \n",
    "    def pi_x(self, ctx):\n",
    "        '''\n",
    "        Return pi(-|x) which is a vector in R^|A|\n",
    "        '''\n",
    "        p = np.zeros(num_actions)\n",
    "        a = model.predict(contexts_X[ctx][np.newaxis,:])[0]\n",
    "        p[a] = 1\n",
    "        return p\n",
    "    \n",
    "    def pi_a_x(self, ctx, a):\n",
    "        '''\n",
    "        Return pi(a|x). \n",
    "        In the case of a deterministic policy should be a binary variable.\n",
    "        '''\n",
    "        a_predict = model.predict(contexts_X[int(ctx)][np.newaxis,:])[0]\n",
    "        return a_predict == a\n",
    "    \n",
    "    def V(self, r):\n",
    "        '''\n",
    "        Return the true value of the task given the true rewards and \n",
    "        Assume context distribution is uniform.\n",
    "        '''\n",
    "        v = 0\n",
    "        for ctx in range(self.num_contexts):\n",
    "            for action in range(self.num_actions):\n",
    "                v += 1/self.num_contexts*self.pi_a_x(ctx, action)*r[ctx, action]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20a6c8",
   "metadata": {},
   "source": [
    "Now the main exercise. Using the reduction to multi-class classification, implement ``ipw_learner`` below. Use the ``LogisticRegression`` class from ``sklearn`` as your underlying training procedure. I've provided some testing code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b06c1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw_learner(data, contexts_X, contexts_A):\n",
    "    # TODO\n",
    "    # return a LogisticDeterministicPolicy object\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing code to check your implementation of ipw_learner.\n",
    "d, r, pi0, contexts_X, contexts_A = logging_data_logistic(1000, 50, 2, seed=50)\n",
    "learned_policy = ipw_learner(d, contexts_X, contexts_A)\n",
    "v_star = logistic_policy.V(r)\n",
    "print('True value of learned policy', v_star)\n",
    "# Value of true best deterministic policy\n",
    "m = 0\n",
    "for i in range(50):\n",
    "    m += np.max(r[i])\n",
    "print('best deterministic policy', m/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5a102",
   "metadata": {},
   "source": [
    "Finallly run a simulation as described in the text. I have tried to give you some warmup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 20\n",
    "ns = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "runs_ips = np.zeros((len(ns), reps))\n",
    "runs_dm = np.zeros((len(ns), reps))\n",
    "runs_dr = np.zeros((len(ns), reps))\n",
    "\n",
    "\n",
    "d, r, pi0, contexts_X, contexts_A = logging_data_logistic(1000, 50, 2, seed=50)\n",
    "\n",
    "for j, n in enumerate(ns):    \n",
    "    for i in range(reps):\n",
    "        '''\n",
    "        TODO:\n",
    "        a) Draw a dataset of size n from logging_data_tabular\n",
    "        b) Learn a pi_hat from ipw_learner\n",
    "        c) Compute vhat(pi_hat) using IPS, DM, DR estimators\n",
    "        d) Compute the mean squared error for each estimator relative to v_star\n",
    "        e) Store the resulting answer in runs_ips[j,i], runs_dm[j,i], runs_dr[j,i] respectively.\n",
    "        ''' \n",
    "\n",
    "plt.plot(ns, runs_ips.mean(axis=1), label='V_IPS', c='blue')\n",
    "plt.errorbar(ns, runs_ips.mean(axis=1), yerr=runs_ips.std(axis=1), c='blue')\n",
    "plt.plot(ns, runs_dm.mean(axis=1), label='V_dm',c='green')\n",
    "plt.errorbar(ns, runs_dm.mean(axis=1), yerr=runs_dm.std(axis=1),c='green')\n",
    "plt.plot(ns, runs_dr.mean(axis=1), label='V_dm',c='orange')\n",
    "plt.errorbar(ns, runs_dr.mean(axis=1), yerr=runs_dr.std(axis=1),c='orange')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
